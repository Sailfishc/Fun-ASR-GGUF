{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# FunASR-GGUF Pickle Inference Notebook\n",
                "\n",
                "这个 Notebook 用于从保存的 Embedding Pickle 文件中直接进行推理。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "import numpy as np\n",
                "import logging\n",
                "import ctypes\n",
                "import time\n",
                "import os\n",
                "import sys\n",
                "\n",
                "# 尝试添加路径以防找不到模块\n",
                "sys.path.append(os.getcwd())\n",
                "\n",
                "from llama_cpp import (\n",
                "    Llama,\n",
                "    llama_batch_init,\n",
                "    llama_batch_free,\n",
                "    llama_decode,\n",
                "    llama_get_logits,\n",
                "    llama_kv_self_clear,\n",
                ")\n",
                "\n",
                "# 配置日志\n",
                "logging.basicConfig(\n",
                "    level=logging.INFO,\n",
                "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
                "    force=True  # 强制重置日志配置，适配 Notebook 环境\n",
                ")\n",
                "logger = logging.getLogger(__name__)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 常量定义\n",
                "GGUF_MODEL_PATH = r'./model-gguf/qwen3-0.6b-asr.gguf'\n",
                "MAX_SEQ_LEN = 1024\n",
                "STOP_TOKEN = [151643, 151645]\n",
                "MAX_THREADS = 0 # 0 = Auto"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "def decode_with_pure_embeddings(llm_obj, audio_embeddings, max_new_tokens=200):\n",
                "    \"\"\"\n",
                "    纯 Embedding 解码函数 (复用自 Python 脚本)\n",
                "    \"\"\"\n",
                "    \n",
                "    # 1. 准备数据\n",
                "    embeds = audio_embeddings.squeeze()\n",
                "    if len(embeds.shape) == 1:\n",
                "        embeds = embeds.reshape(1, -1)\n",
                "    \n",
                "    n_tokens, n_dim = embeds.shape\n",
                "    logger.info(f\"注入 Embedding Shape: {embeds.shape}\")\n",
                "\n",
                "    # 2. 初始化 Batch\n",
                "    batch_embd = llama_batch_init(n_tokens, n_dim, 1)        \n",
                "    batch_text = llama_batch_init(1, 0, 1)\n",
                "\n",
                "    ctx = llm_obj.ctx\n",
                "    \n",
                "    # 3. 清理上下文缓存\n",
                "    llama_kv_self_clear(llm_obj.ctx) \n",
                "    \n",
                "    try:\n",
                "        # ---------------------------------------------------------------------\n",
                "        # A. 注入 Embedding\n",
                "        # ---------------------------------------------------------------------\n",
                "        logger.info(\"正在注入 Embedding...\")\n",
                "        \n",
                "        batch_embd.n_tokens = n_tokens\n",
                "        llm_obj.n_tokens = 0 \n",
                "        \n",
                "        # 关键：batch.token 设置为 NULL\n",
                "        batch_embd.token = ctypes.cast(None, ctypes.POINTER(ctypes.c_int32))\n",
                "\n",
                "        for i in range(n_tokens):\n",
                "            batch_embd.pos[i] = i\n",
                "            batch_embd.n_seq_id[i] = 1\n",
                "            batch_embd.seq_id[i][0] = 0\n",
                "            batch_embd.logits[i] = 1 if i == n_tokens - 1 else 0\n",
                "\n",
                "        if not embeds.flags['C_CONTIGUOUS']:\n",
                "            embeds = np.ascontiguousarray(embeds)\n",
                "        \n",
                "        ctypes.memmove(batch_embd.embd, embeds.ctypes.data, embeds.nbytes)\n",
                "        \n",
                "        if llama_decode(ctx, batch_embd) != 0:\n",
                "             raise RuntimeError(\"Audio embedding decoding failed\")\n",
                "        \n",
                "        llm_obj.n_tokens += n_tokens\n",
                "\n",
                "        # ---------------------------------------------------------------------\n",
                "        # B. 文本生成\n",
                "        # ---------------------------------------------------------------------\n",
                "        generated_text = \"\"\n",
                "        logger.info(f\"开始生成文本...\\n\")\n",
                "        \n",
                "        eos_token = llm_obj.token_eos()\n",
                "        vocab_size = llm_obj.n_vocab()\n",
                "        \n",
                "        batch_text.n_tokens = 1\n",
                "        \n",
                "        gen_start_time = time.time()\n",
                "        tokens_generated = 0\n",
                "        \n",
                "        for step in range(max_new_tokens):\n",
                "            logits_ptr = llama_get_logits(ctx)\n",
                "            logits_arr = np.ctypeslib.as_array(logits_ptr, shape=(vocab_size,))\n",
                "            token_id = int(np.argmax(logits_arr))\n",
                "            \n",
                "            if token_id == eos_token or token_id in STOP_TOKEN:\n",
                "                break\n",
                "                \n",
                "            try:\n",
                "                text_piece = llm_obj.detokenize([token_id]).decode('utf-8', errors='ignore')\n",
                "                print(text_piece, end=\"\", flush=True)\n",
                "                generated_text += text_piece\n",
                "                tokens_generated += 1\n",
                "            except Exception:\n",
                "                pass\n",
                "                \n",
                "            batch_text.token[0] = token_id\n",
                "            batch_text.pos[0] = llm_obj.n_tokens\n",
                "            batch_text.n_seq_id[0] = 1\n",
                "            batch_text.seq_id[0][0] = 0\n",
                "            batch_text.logits[0] = 1\n",
                "            \n",
                "            if llama_decode(ctx, batch_text) != 0:\n",
                "                break\n",
                "            \n",
                "            llm_obj.n_tokens += 1\n",
                "            \n",
                "        print('\\n\\n\\n')\n",
                "        gen_duration = time.time() - gen_start_time\n",
                "        tps = tokens_generated / gen_duration if gen_duration > 0 else 0\n",
                "        logger.info(f\"解码速度: {tps:.2f} tokens/s ({tokens_generated} tokens in {gen_duration:.2f}s)\\n\")\n",
                "        \n",
                "    finally:\n",
                "        llama_batch_free(batch_embd)\n",
                "        llama_batch_free(batch_text)\n",
                "\n",
                "    return generated_text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 在这里加载模型 (只需运行一次))\n",
                "print(f'Loading GGUF model: {GGUF_MODEL_PATH}')\n",
                "llm = Llama(\n",
                "    model_path=GGUF_MODEL_PATH,\n",
                "    n_ctx=MAX_SEQ_LEN + 1024,\n",
                "    n_threads=MAX_THREADS,\n",
                "    embedding=True,\n",
                "    verbose=False\n",
                ")\n",
                "print('GGUF model loaded successfully!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-19 17:40:46,746 - INFO - 注入 Embedding Shape: (155, 1024)\n",
                        "2026-01-19 17:40:46,759 - INFO - 正在注入 Embedding...\n",
                        "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processing: ./pickles/embedding_slice_0_160000.pkl\n",
                        "Loaded embeddings shape: (155, 1024)\n",
                        "\n",
                        "--- Result ---\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-19 17:40:47,409 - INFO - 开始生成文本...\n",
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "，星期日，欢迎收看一千零四期誓言消息，请静静介绍话题。去年十月十九日，九百六十七期节目说到委内瑞拉问题，我们回顾一下你当时的评。\n",
                        "\n",
                        "\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-19 17:40:48,543 - INFO - 解码速度: 39.69 tokens/s (45 tokens in 1.13s)\n",
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- End ---\n"
                    ]
                }
            ],
            "source": [
                "# === 指定要转录的 Pickle 文件 ===\n",
                "# 修改下面的路径为你想要测试的文件\n",
                "TARGET_PICKLE = r'./pickles/embedding_slice_0_160000.pkl' \n",
                "\n",
                "# 自动查找最新 (可选)\n",
                "if not os.path.exists(TARGET_PICKLE) and os.path.exists(\"pickles\"):\n",
                "    files = [os.path.join(\"pickles\", f) for f in os.listdir(\"pickles\") if f.endswith(\".pkl\")]\n",
                "    if files:\n",
                "        TARGET_PICKLE = max(files, key=os.path.getctime)\n",
                "        print(f\"Auto-selected latest file: {TARGET_PICKLE}\")\n",
                "\n",
                "print(f\"Processing: {TARGET_PICKLE}\")\n",
                "\n",
                "if os.path.exists(TARGET_PICKLE):\n",
                "    with open(TARGET_PICKLE, 'rb') as f:\n",
                "        embeddings_data = pickle.load(f)\n",
                "    \n",
                "    print(f\"Loaded embeddings shape: {embeddings_data.shape}\")\n",
                "    print(\"\\n--- Result ---\")\n",
                "    result = decode_with_pure_embeddings(llm, embeddings_data, max_new_tokens=MAX_SEQ_LEN)\n",
                "    print(\"\\n--- End ---\")\n",
                "else:\n",
                "    print(f\"Error: File {TARGET_PICKLE} not found.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "funasr",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
